"""
Excelæ•°æ®å¤„ç†å™¨ - å¤§å¸ˆçº§å®ç°
ä¼˜é›…ã€ç²¾ç®€ã€é«˜æ€§èƒ½çš„Excelæ–‡ä»¶æ‰¹é‡å¤„ç†å·¥å…·
"""

from pathlib import Path
from typing import Iterator, Optional, Dict, List, Any, Tuple, Set
from dataclasses import dataclass
from functools import wraps
from collections import defaultdict
import logging

import pandas as pd

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


@dataclass
class ProcessResult:
    """å¤„ç†ç»“æœæ•°æ®ç±»"""
    data: pd.DataFrame
    files_processed: int
    sheets_processed: int
    total_rows: int
    column_sources: Dict[str, Set[Tuple[str, str]]]
    processing_stats: Dict[str, Any]
    scan_summary: Dict[str, Any]


def handle_errors(func):
    """é”™è¯¯å¤„ç†è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"âŒ {func.__name__}: {e}")
            return None
    return wrapper


def find_target_row(df: pd.DataFrame, target: str = "èº«ä»½è¯å·") -> Optional[int]:
    """åœ¨å‰10è¡Œä¸­æŸ¥æ‰¾åŒ…å«ç›®æ ‡å­—ç¬¦ä¸²çš„è¡Œ"""
    for i in range(min(10, len(df))):
        if df.iloc[i].astype(str).str.contains(target, na=False).any():
            return i
    return None


def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """æ•°æ®æ¸…æ´— - é“¾å¼æ“ä½œ"""
    return (df
            .dropna(how='all')  # åˆ é™¤ç©ºè¡Œ
            .dropna(axis=1, how='all')  # åˆ é™¤ç©ºåˆ—
            .loc[:, ~df.apply(lambda x: x.astype(str).str.strip().eq('').all())]  # åˆ é™¤ç©ºå­—ç¬¦ä¸²åˆ—
            .pipe(lambda x: x[~x.apply(lambda row: row.astype(str).str.strip().eq('').all(), axis=1)])  # åˆ é™¤ç©ºå­—ç¬¦ä¸²è¡Œ
            .reset_index(drop=True))


def create_headers(row_data: pd.Series) -> List[str]:
    """åˆ›å»ºå”¯ä¸€åˆ—å"""
    headers = []
    for i, val in enumerate(row_data):
        name = str(val) if pd.notna(val) else f"æœªå‘½å_{i}"
        # å¤„ç†é‡å¤åˆ—å
        original = name
        counter = 1
        while name in headers:
            name = f"{original}_{counter}"
            counter += 1
        headers.append(name)
    return headers


@handle_errors
def process_sheet(file_path: Path, sheet_name: str) -> Optional[pd.DataFrame]:
    """å¤„ç†å•ä¸ªå·¥ä½œè¡¨"""
    df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)

    if df.empty:
        return None

    # æŸ¥æ‰¾ç›®æ ‡è¡Œ
    target_row = find_target_row(df)
    if target_row is None:
        return None

    # é‡æ„æ•°æ®
    headers = create_headers(df.iloc[target_row])
    data = df.iloc[target_row + 1:].copy()
    data.columns = headers

    # æ¸…æ´—å¹¶æ·»åŠ å…ƒä¿¡æ¯
    return (clean_data(data)
            .assign(æ–‡ä»¶å=file_path.name, å·¥ä½œè¡¨å=sheet_name)
            .pipe(lambda x: x if not x.empty else None))


def process_file(file_path: Path) -> Tuple[List[pd.DataFrame], Dict[str, Any]]:
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„æ‰€æœ‰å·¥ä½œè¡¨"""
    relative_path = file_path.relative_to(file_path.parent.parent)
    logger.info(f"\nğŸ“‚ å¤„ç†æ–‡ä»¶: {relative_path}")
    logger.info("â”€" * 60)

    file_stats = {
        'file_name': file_path.name,
        'relative_path': str(relative_path),
        'sheets_processed': 0,
        'sheets_skipped': 0,
        'total_data_rows': 0,
        'sheet_details': []
    }

    try:
        with pd.ExcelFile(file_path) as excel_file:
            sheet_names = excel_file.sheet_names
            file_stats['total_sheets'] = len(sheet_names)

            logger.info(f"   ğŸ“Š å‘ç° {len(sheet_names)} ä¸ªå·¥ä½œè¡¨: {', '.join(sheet_names)}")

            processed_dfs = []
            for sheet_idx, sheet_name in enumerate(sheet_names, 1):
                check_msg = f"   â””â”€ [{sheet_idx}/{len(sheet_names)}] ğŸ” æ£€æŸ¥å·¥ä½œè¡¨: '{sheet_name}' "

                result = process_sheet(file_path, sheet_name)

                sheet_detail = {
                    'sheet_name': sheet_name,
                    'has_target': False,
                    'data_rows': 0,
                    'columns_count': 0
                }

                if result is not None:
                    processed_dfs.append(result)
                    file_stats['sheets_processed'] += 1
                    file_stats['total_data_rows'] += len(result)

                    data_columns = [col for col in result.columns if col not in ['æ–‡ä»¶å', 'å·¥ä½œè¡¨å']]
                    sheet_detail.update({
                        'has_target': True,
                        'data_rows': len(result),
                        'columns_count': len(data_columns)
                    })

                    logger.info(check_msg + f"âœ… (æ‰¾åˆ°ç›®æ ‡å­—ç¬¦ä¸²ï¼Œè·å¾— {len(result)} è¡Œæ•°æ®)")
                else:
                    file_stats['sheets_skipped'] += 1
                    logger.info(check_msg + "âŒ (æœªæ‰¾åˆ°åŒ…å«'èº«ä»½è¯å·'çš„è¡Œ)")

                file_stats['sheet_details'].append(sheet_detail)

            if file_stats['sheets_processed'] > 0:
                logger.info(f"   ğŸ“ˆ æ–‡ä»¶æ±‡æ€»: æˆåŠŸå¤„ç† {file_stats['sheets_processed']} ä¸ªå·¥ä½œè¡¨ï¼Œå…± {file_stats['total_data_rows']} è¡Œæ•°æ®")
            else:
                logger.info("   âš ï¸  æ–‡ä»¶æ±‡æ€»: è¯¥æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®")

            return processed_dfs, file_stats

    except Exception as e:
        logger.warning(f"   âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        error_info = {
            'file_name': file_path.name,
            'relative_path': str(relative_path),
            'error': str(e)
        }
        return [], {'error': error_info, **file_stats}


def scan_directory_structure(folder_path: Path) -> Tuple[List[Path], Dict[str, Any]]:
    """æ‰«æç›®å½•ç»“æ„ï¼Œè·å–å®Œæ•´ç»Ÿè®¡ä¿¡æ¯"""
    logger.info(f"ğŸ“ å¼€å§‹æ‰«æç›®å½•ç»“æ„: {folder_path}")

    # æ‰«ææ‰€æœ‰å­æ–‡ä»¶å¤¹
    subdirs = [d for d in folder_path.iterdir() if d.is_dir()]

    # æ‰«ææ‰€æœ‰Excelæ–‡ä»¶ï¼ˆä¸é™åˆ¶æ–‡ä»¶åï¼‰
    all_excel_files = list(folder_path.glob("*/*.xlsx")) + list(folder_path.glob("*/*.xls"))

    # åŒ¹é…ç›®æ ‡æ–‡ä»¶ï¼ˆä»¥"ä¼˜"å¼€å¤´çš„xlsxæ–‡ä»¶ï¼‰
    target_files = list(folder_path.glob("*/ä¼˜*.xlsx")) + list(folder_path.glob("*/ä¼˜*.xls"))

    # æŒ‰å­æ–‡ä»¶å¤¹åˆ†ç»„ç»Ÿè®¡
    files_by_subdir = defaultdict(lambda: {'all': [], 'target': []})
    for file_path in all_excel_files:
        subdir_name = file_path.parent.name
        files_by_subdir[subdir_name]['all'].append(file_path)
        if file_path.name.startswith('ä¼˜') and (file_path.suffix == '.xlsx' or file_path.suffix == '.xls'):
            files_by_subdir[subdir_name]['target'].append(file_path)

    scan_summary = {
        'total_subdirs': len(subdirs),
        'subdirs_with_excel': len(files_by_subdir),
        'all_excel_files': len(all_excel_files),
        'target_excel_files': len(target_files),
        'subdirs_detail': dict(files_by_subdir),
        'subdir_names': [d.name for d in subdirs]
    }

    # è¯¦ç»†æ—¥å¿—è¾“å‡º
    logger.info(f"ğŸ“Š ç›®å½•æ‰«æç»“æœ:")
    logger.info(f"   â€¢ æ€»å­æ–‡ä»¶å¤¹æ•°: {scan_summary['total_subdirs']}")
    logger.info(f"   â€¢ åŒ…å«Excelçš„å­æ–‡ä»¶å¤¹: {scan_summary['subdirs_with_excel']}")
    logger.info(f"   â€¢ æ‰€æœ‰Excelæ–‡ä»¶: {scan_summary['all_excel_files']} ä¸ª")
    logger.info(f"   â€¢ ç›®æ ‡Excelæ–‡ä»¶: {scan_summary['target_excel_files']} ä¸ª")

    if scan_summary['subdirs_with_excel'] > 0:
        logger.info(f"ğŸ“‹ å„å­æ–‡ä»¶å¤¹Excelæ–‡ä»¶åˆ†å¸ƒ:")
        for subdir, files_info in files_by_subdir.items():
            all_count = len(files_info['all'])
            target_count = len(files_info['target'])
            logger.info(f"   â€¢ {subdir}: {all_count} ä¸ªExcelæ–‡ä»¶ (å…¶ä¸­ {target_count} ä¸ªç›®æ ‡æ–‡ä»¶)")

    if scan_summary['all_excel_files'] > scan_summary['target_excel_files']:
        missed_files = scan_summary['all_excel_files'] - scan_summary['target_excel_files']
        logger.info(f"âš ï¸  å‘ç° {missed_files} ä¸ªExcelæ–‡ä»¶ä¸åŒ¹é…å¤„ç†æ¡ä»¶ï¼ˆé'ä¼˜'å¼€å¤´çš„xlsxæ–‡ä»¶ï¼‰")

    return target_files, scan_summary

def get_excel_files(folder_path: Path) -> Tuple[List[Path], Dict[str, Any]]:
    """è·å–ç›®æ ‡Excelæ–‡ä»¶å’Œæ‰«ææ‘˜è¦"""
    logger.info("ğŸ”„ æœç´¢Excelæ–‡ä»¶...")
    target_files, scan_summary = scan_directory_structure(folder_path)

    if not target_files:
        raise FileNotFoundError("âŒ åœ¨æŒ‡å®šè·¯å¾„çš„å­æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°ä»¥'ä¼˜'å­—å¼€å¤´çš„xlsxæ–‡ä»¶")

    logger.info(f"âœ… æ‰¾åˆ° {len(target_files)} ä¸ªåŒ¹é…çš„ç›®æ ‡æ–‡ä»¶:")
    for i, file_path in enumerate(target_files, 1):
        relative_path = file_path.relative_to(folder_path)
        logger.info(f"   {i:2d}. {relative_path}")

    logger.info("âœ… æœç´¢Excelæ–‡ä»¶ å®Œæˆ")
    return target_files, scan_summary


def unify_columns(dataframes: List[pd.DataFrame]) -> List[pd.DataFrame]:
    """ç»Ÿä¸€åˆ—ç»“æ„"""
    if not dataframes:
        return []

    # è·å–æ‰€æœ‰åˆ—çš„å¹¶é›†
    all_columns = set()
    for df in dataframes:
        all_columns.update(col for col in df.columns if col not in ['æ–‡ä»¶å', 'å·¥ä½œè¡¨å'])

    unified_columns = sorted(all_columns) + ['æ–‡ä»¶å', 'å·¥ä½œè¡¨å']

    # ç»Ÿä¸€æ‰€æœ‰DataFrameçš„åˆ—
    logger.info(f"ğŸ“‹ ç»Ÿä¸€åˆ—ç»“æ„: å…± {len(unified_columns) - 2} ä¸ªæ•°æ®åˆ—")
    return [df.reindex(columns=unified_columns, fill_value=None) for df in dataframes]


def collect_column_sources(dataframes: List[pd.DataFrame]) -> Dict[str, Set[Tuple[str, str]]]:
    """æ”¶é›†åˆ—æ¥æºä¿¡æ¯"""
    column_sources = defaultdict(set)

    for df in dataframes:
        data_columns = [col for col in df.columns if col not in ['æ–‡ä»¶å', 'å·¥ä½œè¡¨å']]
        if not df.empty:
            file_name = df['æ–‡ä»¶å'].iloc[0]
            sheet_name = df['å·¥ä½œè¡¨å'].iloc[0]

            for col in data_columns:
                column_sources[col].add((file_name, sheet_name))

    return column_sources


def log_column_sources(column_sources: Dict[str, Set[Tuple[str, str]]]) -> None:
    """è®°å½•åˆ—æ¥æºä¿¡æ¯"""
    data_columns = [col for col in column_sources.keys() if col not in ['æ–‡ä»¶å', 'å·¥ä½œè¡¨å']]
    logger.info(f"ğŸ“‹ å‘ç° {len(data_columns)} ä¸ªä¸åŒçš„æ•°æ®åˆ—åŠå…¶æ¥æº:")

    for i, col in enumerate(sorted(data_columns), 1):
        sources = column_sources[col]
        if len(sources) == 1:
            source_info = list(sources)[0]
            logger.info(f"   {i:2d}. {col}")
            logger.info(f"       â””â”€ æ¥æº: {source_info[0]} -> {source_info[1]}")
        else:
            logger.info(f"   {i:2d}. {col}")
            logger.info(f"       â””â”€ æ¥æºäº {len(sources)} ä¸ªå·¥ä½œè¡¨:")
            for source in sorted(sources):
                logger.info(f"          â€¢ {source[0]} -> {source[1]}")


def calculate_completeness_stats(df: pd.DataFrame) -> List[Dict]:
    """è®¡ç®—æ•°æ®å®Œæ•´æ€§ç»Ÿè®¡"""
    data_cols = [col for col in df.columns if col not in ['æ–‡ä»¶å', 'å·¥ä½œè¡¨å']]
    completeness_stats = []

    for col in data_cols:
        non_null_count = df[col].notna().sum()
        completeness_rate = (non_null_count / len(df)) * 100
        completeness_stats.append({
            'column': col,
            'non_null_count': non_null_count,
            'completeness_rate': completeness_rate
        })

    return sorted(completeness_stats, key=lambda x: x['completeness_rate'], reverse=True)


def log_completeness_analysis(completeness_stats: List[Dict]) -> None:
    """è®°å½•æ•°æ®å®Œæ•´æ€§åˆ†æ"""
    logger.info(f"\nğŸ” æ•°æ®å®Œæ•´æ€§åˆ†æ:")
    logger.info("   åˆ—å\t\t\t\téç©ºå€¼æ•°é‡\tå®Œæ•´ç‡")
    logger.info("   " + "-" * 60)

    for stat in completeness_stats:
        col_name = stat['column']
        # æ ¹æ®åˆ—åé•¿åº¦è°ƒæ•´tabæ•°é‡
        if len(col_name) < 8:
            tabs = "\t\t\t\t"
        elif len(col_name) < 16:
            tabs = "\t\t\t"
        elif len(col_name) < 24:
            tabs = "\t\t"
        else:
            tabs = "\t"

        logger.info(f"   {col_name}{tabs}{stat['non_null_count']:,}\t\t{stat['completeness_rate']:.1f}%")


def log_processing_summary(processing_stats: Dict[str, Any], final_df: pd.DataFrame) -> None:
    """è®°å½•å¤„ç†æ€»ç»“ä¿¡æ¯"""
    logger.info(f"\n" + "=" * 80)
    logger.info("ğŸ“Š å¤„ç†å®Œæˆ - è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š")
    logger.info("=" * 80)

    logger.info("ğŸ“ æ–‡ä»¶å¤„ç†ç»Ÿè®¡:")
    logger.info(f"   â€¢ æ€»æ–‡ä»¶æ•°: {processing_stats['total_files']}")
    logger.info(f"   â€¢ æˆåŠŸå¤„ç†: {processing_stats['processed_files']} ä¸ªæ–‡ä»¶")
    logger.info(f"   â€¢ è·³è¿‡æ–‡ä»¶: {processing_stats['skipped_files']} ä¸ªæ–‡ä»¶")
    if processing_stats.get('error_files'):
        logger.info(f"   â€¢ é”™è¯¯æ–‡ä»¶: {len(processing_stats['error_files'])} ä¸ªæ–‡ä»¶")

    logger.info(f"\nğŸ“Š å·¥ä½œè¡¨å¤„ç†ç»Ÿè®¡:")
    logger.info(f"   â€¢ æ€»å·¥ä½œè¡¨æ•°: {processing_stats['total_sheets']}")
    logger.info(f"   â€¢ æˆåŠŸå¤„ç†: {processing_stats['processed_sheets']} ä¸ªå·¥ä½œè¡¨")
    logger.info(f"   â€¢ è·³è¿‡å·¥ä½œè¡¨: {processing_stats['skipped_sheets']} ä¸ªå·¥ä½œè¡¨")

    logger.info(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    logger.info(f"   â€¢ æ€»è¡Œæ•°: {len(final_df):,}")
    logger.info(f"   â€¢ æ€»åˆ—æ•°: {len(final_df.columns)}")
    logger.info(f"   â€¢ æ•°æ®åˆ—æ•°: {len(final_df.columns) - 2}")
    logger.info(f"   â€¢ æ¥æºä¿¡æ¯åˆ—: 2 (æ–‡ä»¶å, å·¥ä½œè¡¨å)")

    # æŒ‰æ–‡ä»¶åˆ†ç»„ç»Ÿè®¡
    logger.info(f"\nğŸ“‹ æŒ‰æ–‡ä»¶è¯¦ç»†ç»Ÿè®¡:")
    for detail in processing_stats.get('files_details', []):
        if detail.get('sheets_processed', 0) > 0:
            logger.info(f"   ğŸ“‚ {detail['relative_path']}")
            logger.info(f"      â””â”€ å¤„ç†å·¥ä½œè¡¨: {detail['sheets_processed']}/{detail['total_sheets']}")
            logger.info(f"      â””â”€ æ•°æ®è¡Œæ•°: {detail['total_data_rows']:,}")

    # æŒ‰æ¥æºç»Ÿè®¡æ•°æ®åˆ†å¸ƒ
    logger.info(f"\nğŸ—‚ï¸  æ•°æ®æ¥æºåˆ†å¸ƒ:")
    source_stats = final_df.groupby(['æ–‡ä»¶å', 'å·¥ä½œè¡¨å']).size().reset_index()
    source_stats.columns = ['æ–‡ä»¶å', 'å·¥ä½œè¡¨å', 'è¡Œæ•°']
    for _, row in source_stats.iterrows():
        logger.info(f"   â€¢ {row['æ–‡ä»¶å']} -> {row['å·¥ä½œè¡¨å']}: {row['è¡Œæ•°']:,} è¡Œ")

    # é”™è¯¯æ–‡ä»¶è¯¦æƒ…
    if processing_stats.get('error_files'):
        logger.info(f"\nâŒ é”™è¯¯æ–‡ä»¶è¯¦æƒ…:")
        for error in processing_stats['error_files']:
            logger.warning(f"   â€¢ {error['relative_path']}: {error['error']}")


def log_comprehensive_summary(scan_summary: Dict[str, Any], processing_stats: Dict[str, Any], final_df: pd.DataFrame) -> None:
    """è®°å½•å…¨é¢çš„å¤„ç†æ€»ç»“ï¼Œç¡®ä¿æ— é—æ¼"""
    logger.info(f"\n" + "=" * 80)
    logger.info("ğŸ¯ å…¨é¢æ‰«æä¸å¤„ç†æ€»ç»“ - ç¡®ä¿é›¶é—æ¼")
    logger.info("=" * 80)

    # æ‰«æè¦†ç›–ç‡
    logger.info("ğŸ“ ç›®å½•æ‰«æè¦†ç›–ç‡:")
    logger.info(f"   â€¢ æ‰«ææ ¹ç›®å½•: 1 ä¸ª")
    logger.info(f"   â€¢ å‘ç°å­æ–‡ä»¶å¤¹: {scan_summary['total_subdirs']} ä¸ª")
    logger.info(f"   â€¢ åŒ…å«Excelçš„å­æ–‡ä»¶å¤¹: {scan_summary['subdirs_with_excel']} ä¸ª")

    if scan_summary['total_subdirs'] > 0:
        coverage_rate = (scan_summary['subdirs_with_excel'] / scan_summary['total_subdirs']) * 100
        logger.info(f"   â€¢ å­æ–‡ä»¶å¤¹è¦†ç›–ç‡: {coverage_rate:.1f}%")

    # æ–‡ä»¶æ‰«æè¯¦æƒ…
    logger.info(f"\nğŸ“Š æ–‡ä»¶æ‰«æè¯¦æƒ…:")
    logger.info(f"   â€¢ å…¨éƒ¨Excelæ–‡ä»¶æ‰«æ: {scan_summary['all_excel_files']} ä¸ª")
    logger.info(f"   â€¢ ç¬¦åˆæ¡ä»¶çš„ç›®æ ‡æ–‡ä»¶: {scan_summary['target_excel_files']} ä¸ª")
    logger.info(f"   â€¢ ä¸ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶: {scan_summary['all_excel_files'] - scan_summary['target_excel_files']} ä¸ª")

    if scan_summary['all_excel_files'] > 0:
        match_rate = (scan_summary['target_excel_files'] / scan_summary['all_excel_files']) * 100
        logger.info(f"   â€¢ æ–‡ä»¶åŒ¹é…ç‡: {match_rate:.1f}%")

    # å¤„ç†è¦†ç›–ç‡åˆ†æ
    logger.info(f"\nğŸ¯ å¤„ç†è¦†ç›–ç‡åˆ†æ:")
    logger.info(f"   â€¢ ç›®æ ‡æ–‡ä»¶æ•°: {processing_stats['total_files']}")
    logger.info(f"   â€¢ æˆåŠŸå¤„ç†æ–‡ä»¶: {processing_stats['processed_files']}")
    logger.info(f"   â€¢ è·³è¿‡/å¤±è´¥æ–‡ä»¶: {processing_stats['skipped_files']}")

    if processing_stats['total_files'] > 0:
        success_rate = (processing_stats['processed_files'] / processing_stats['total_files']) * 100
        logger.info(f"   â€¢ æ–‡ä»¶å¤„ç†æˆåŠŸç‡: {success_rate:.1f}%")

    # å·¥ä½œè¡¨çº§åˆ«è¦†ç›–ç‡
    logger.info(f"\nğŸ“‹ å·¥ä½œè¡¨çº§åˆ«è¦†ç›–ç‡:")
    logger.info(f"   â€¢ å‘ç°å·¥ä½œè¡¨æ€»æ•°: {processing_stats['total_sheets']}")
    logger.info(f"   â€¢ åŒ…å«ç›®æ ‡æ•°æ®çš„å·¥ä½œè¡¨: {processing_stats['processed_sheets']}")
    logger.info(f"   â€¢ ä¸åŒ…å«ç›®æ ‡æ•°æ®çš„å·¥ä½œè¡¨: {processing_stats['skipped_sheets']}")

    if processing_stats['total_sheets'] > 0:
        sheet_success_rate = (processing_stats['processed_sheets'] / processing_stats['total_sheets']) * 100
        logger.info(f"   â€¢ å·¥ä½œè¡¨æ•°æ®æå–æˆåŠŸç‡: {sheet_success_rate:.1f}%")

    # æ•°æ®å®Œæ•´æ€§ä¿è¯
    logger.info(f"\nâœ… æ•°æ®å®Œæ•´æ€§ä¿è¯:")
    logger.info(f"   â€¢ æœ€ç»ˆæ•°æ®è¡Œæ•°: {len(final_df):,}")
    logger.info(f"   â€¢ æ•°æ®æ¥æºè¿½è¸ª: å®Œæ•´ä¿ç•™æ–‡ä»¶åå’Œå·¥ä½œè¡¨å")
    logger.info(f"   â€¢ åˆ—åˆå¹¶ç­–ç•¥: æ‰€æœ‰åˆ—åšå¹¶é›†ï¼Œç¼ºå¤±å€¼ç”¨NaNå¡«å……")

    # æ½œåœ¨é—æ¼æé†’
    if scan_summary['all_excel_files'] > scan_summary['target_excel_files']:
        missed_count = scan_summary['all_excel_files'] - scan_summary['target_excel_files']
        logger.info(f"\nâš ï¸  æ½œåœ¨é—æ¼æé†’:")
        logger.info(f"   â€¢ å‘ç° {missed_count} ä¸ªExcelæ–‡ä»¶æœªå¤„ç†ï¼ˆä¸ç¬¦åˆ'ä¼˜'å¼€å¤´æ¡ä»¶ï¼‰")
        logger.info(f"   â€¢ å¦‚éœ€å¤„ç†è¿™äº›æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å‘½åæˆ–è°ƒæ•´æœç´¢æ¡ä»¶")

    if processing_stats['skipped_files'] > 0:
        logger.info(f"\nâš ï¸  å¤„ç†å¼‚å¸¸æ–‡ä»¶:")
        logger.info(f"   â€¢ {processing_stats['skipped_files']} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥æˆ–è·³è¿‡")
        logger.info(f"   â€¢ è¯·æ£€æŸ¥é”™è¯¯è¯¦æƒ…ç¡®ä¿æ— é‡è¦æ•°æ®é—æ¼")

    if processing_stats['skipped_sheets'] > 0:
        logger.info(f"\nâš ï¸  æœªæå–æ•°æ®çš„å·¥ä½œè¡¨:")
        logger.info(f"   â€¢ {processing_stats['skipped_sheets']} ä¸ªå·¥ä½œè¡¨æœªæ‰¾åˆ°ç›®æ ‡å­—ç¬¦ä¸²'èº«ä»½è¯å·'")
        logger.info(f"   â€¢ å¦‚è¿™äº›å·¥ä½œè¡¨åŒ…å«é‡è¦æ•°æ®ï¼Œè¯·æ£€æŸ¥è¡¨å¤´è®¾ç½®")

    # æœ€ç»ˆç¡®è®¤
    logger.info(f"\nğŸ”’ å¤„ç†å®Œæ•´æ€§ç¡®è®¤:")
    total_possible_data = scan_summary['target_excel_files']
    actually_processed = processing_stats['processed_files']

    if actually_processed == total_possible_data:
        logger.info(f"   âœ… å®Œç¾å¤„ç†: æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶å‡å·²æˆåŠŸå¤„ç†")
    else:
        logger.info(f"   âš ï¸  å¤„ç†ç‡: {actually_processed}/{total_possible_data} ä¸ªæ–‡ä»¶è¢«å¤„ç†")
        logger.info(f"   ğŸ’¡ å»ºè®®æ£€æŸ¥å¤±è´¥æ–‡ä»¶ç¡®ä¿æ— é‡è¦æ•°æ®é—æ¼")


def create_summary(result: ProcessResult) -> Dict[str, Any]:
    """åˆ›å»ºå¤„ç†æ‘˜è¦"""
    df = result.data
    return {
        'files_processed': result.files_processed,
        'sheets_processed': result.sheets_processed,
        'total_rows': result.total_rows,
        'total_columns': len(df.columns),
        'data_columns': len(df.columns) - 2,
        'completeness': {
            col: f"{df[col].notna().sum() / len(df) * 100:.1f}%"
            for col in df.columns if col not in ['æ–‡ä»¶å', 'å·¥ä½œè¡¨å']
        },
        'sources': df.groupby(['æ–‡ä»¶å', 'å·¥ä½œè¡¨å']).size().to_dict()
    }


def process_excel_files(folder_path: str) -> ProcessResult:
    """
    ä¸»å¤„ç†å‡½æ•° - å¤§å¸ˆçº§å®ç°withè¯¦ç»†æ—¥å¿—

    Args:
        folder_path: Excelæ–‡ä»¶å¤¹è·¯å¾„

    Returns:
        ProcessResult: åŒ…å«å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
    """
    logger.info("=" * 80)
    logger.info("ğŸ” å¼€å§‹å¤„ç†Excelæ–‡ä»¶...")
    logger.info("=" * 80)

    path = Path(folder_path)
    excel_files, scan_summary = get_excel_files(path)

    logger.info("\n" + "=" * 80)
    logger.info("ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶...")
    logger.info("=" * 80)

    # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
    processing_stats = {
        'total_files': len(excel_files),
        'processed_files': 0,
        'skipped_files': 0,
        'total_sheets': 0,
        'processed_sheets': 0,
        'skipped_sheets': 0,
        'files_details': [],
        'error_files': []
    }

    # å¤„ç†æ‰€æœ‰æ–‡ä»¶
    all_dataframes = []
    for file_path in excel_files:
        file_dfs, file_stats = process_file(file_path)

        if 'error' in file_stats:
            processing_stats['error_files'].append(file_stats['error'])
            processing_stats['skipped_files'] += 1
            print(file_path)
        else:
            all_dataframes.extend(file_dfs)
            processing_stats['total_sheets'] += file_stats.get('total_sheets', 0)
            processing_stats['processed_sheets'] += file_stats.get('sheets_processed', 0)
            processing_stats['skipped_sheets'] += file_stats.get('sheets_skipped', 0)

            if file_stats.get('sheets_processed', 0) > 0:
                processing_stats['processed_files'] += 1
            else:
                processing_stats['skipped_files'] += 1
                print(file_path)

            processing_stats['files_details'].append(file_stats)

    if not all_dataframes:
        logger.error("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®")
        raise ValueError("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®")

    # åˆå¹¶æ•°æ®
    logger.info(f"\n" + "=" * 80)
    logger.info("ğŸ”— å¼€å§‹åˆå¹¶æ•°æ®...")
    logger.info("=" * 80)

    # æ”¶é›†åˆ—æ¥æºä¿¡æ¯
    column_sources = collect_column_sources(all_dataframes)
    log_column_sources(column_sources)

    # ç»Ÿä¸€åˆ—ç»“æ„å¹¶åˆå¹¶
    unified_dfs = unify_columns(all_dataframes)
    final_df = pd.concat(unified_dfs, ignore_index=True)

    # è®¡ç®—å®Œæ•´æ€§ç»Ÿè®¡
    completeness_stats = calculate_completeness_stats(final_df)

    # åˆ›å»ºç»“æœ
    result = ProcessResult(
        data=final_df,
        files_processed=processing_stats['processed_files'],
        sheets_processed=processing_stats['processed_sheets'],
        total_rows=len(final_df),
        column_sources=column_sources,
        processing_stats=processing_stats,
        scan_summary=scan_summary
    )

    # è®°å½•è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    log_processing_summary(processing_stats, final_df)
    log_completeness_analysis(completeness_stats)

    # è®°å½•å…¨é¢æ€»ç»“ï¼Œç¡®ä¿é›¶é—æ¼
    log_comprehensive_summary(scan_summary, processing_stats, final_df)

    logger.info(f"\n" + "=" * 80)
    logger.info("ğŸ‰ å¤„ç†å®Œæˆï¼æ•°æ®å·²å‡†å¤‡å°±ç»ªã€‚")
    logger.info("=" * 80)

    return result


def save_excel(df: pd.DataFrame, output_path: str) -> None:
    """ä¿å­˜ä¸ºExcelæ–‡ä»¶"""
    try:
        df.to_excel(output_path, index=False, engine='openpyxl')
        logger.info(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")


def main() -> Optional[pd.DataFrame]:
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    try:
        folder_path = input("ğŸ“ Excelæ–‡ä»¶å¤¹è·¯å¾„: ").strip()

        # å¤„ç†æ•°æ®
        result = process_excel_files(folder_path)

        # æ˜¾ç¤ºé¢„è§ˆ
        print("\n" + "="*50)
        print("ğŸ“Š æ•°æ®é¢„è§ˆ (å‰5è¡Œ):")
        print(result.data.head().to_string())

        # ä¿å­˜é€‰é¡¹
        if input("\nğŸ’¾ ä¿å­˜ç»“æœ? (y/N): ").lower() == 'y':
            output_path = input("ğŸ“„ è¾“å‡ºæ–‡ä»¶å: ").strip()
            if not output_path.endswith('.xlsx'):
                output_path += '.xlsx'
            save_excel(result.data, output_path)

        return result.data

    except (KeyboardInterrupt, EOFError):
        logger.info("ğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        return None
    except Exception as e:
        logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
        return None


if __name__ == "__main__":
    main()
